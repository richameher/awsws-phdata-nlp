{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "isConfigCell": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:iam::023375022819:role/service-role/AmazonSageMaker-ExecutionRole-20191220T213935\n",
      "phdata-nlp-s3-bucket\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import json\n",
    "import boto3\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "role = get_execution_role()\n",
    "print(role) \n",
    "\n",
    "bucket = 'phdata-nlp-s3-bucket' \n",
    "print(bucket)\n",
    "prefix = 'blazingtext/supervised' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://phdata-nlp-s3-bucket/labelled_reviews.csv to ./labelled_reviews.csv\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp s3://phdata-nlp-s3-bucket/labelled_reviews.csv `pwd`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ",Review,topic\r\n",
      "0,\"__label__book Buyer beware This is a self-published book, and if you want to know why--read a few paragraphs! Those 5 star reviews must have been written by Ms. Haddon's family and friends--or perhaps, by herself! I can't imagine anyone reading the whole thing--I spent an evening with the book and a friend and we were in hysterics reading bits and pieces of it to one another. It is most definitely bad enough to be entered into some kind of a \"\"worst book\"\" contest. I can't believe Amazon even sells this kind of thing. Maybe I can offer them my 8th grade term paper on \"\"To Kill a Mockingbird\"\"--a book I am quite sure Ms. Haddon never heard of. Anyway, unless you are in a mood to send a book to someone as a joke---stay far, far away from this one!\",book\r\n",
      "1,\"__label__book The Worst! A complete waste of time. Typographical errors, poor grammar, and a totally pathetic plot add up to absolutely nothing. I'm embarrassed for this author and very disappointed I actually paid for this book.\",book\r\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "!head labelled_reviews.csv -n 3\n",
    "\n",
    "reviews = pd.read_csv('labelled_reviews.csv')['Review']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "420001:::179999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/ipykernel/__main__.py:7: FutureWarning: The signature of `Series.to_csv` was aligned to that of `DataFrame.to_csv`, and argument 'header' will change its default value from False to True: please pass an explicit value to suppress this warning.\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/ipykernel/__main__.py:8: FutureWarning: The signature of `Series.to_csv` was aligned to that of `DataFrame.to_csv`, and argument 'header' will change its default value from False to True: please pass an explicit value to suppress this warning.\n"
     ]
    }
   ],
   "source": [
    "#split data into 70% train and 30% test\n",
    "train = reviews[:420001]\n",
    "test = reviews[420001:600000]\n",
    "\n",
    "print(str(len(train) ) + \":::\" + str ( len(test) ) )\n",
    " \n",
    "train.to_csv(\"reviews.train.csv\", index=False)\n",
    "test.to_csv(\"reviews.test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"__label__book Buyer beware This is a self-published book, and if you want to know why--read a few paragraphs! Those 5 star reviews must have been written by Ms. Haddon's family and friends--or perhaps, by herself! I can't imagine anyone reading the whole thing--I spent an evening with the book and a friend and we were in hysterics reading bits and pieces of it to one another. It is most definitely bad enough to be entered into some kind of a \"\"worst book\"\" contest. I can't believe Amazon even sells this kind of thing. Maybe I can offer them my 8th grade term paper on \"\"To Kill a Mockingbird\"\"--a book I am quite sure Ms. Haddon never heard of. Anyway, unless you are in a mood to send a book to someone as a joke---stay far, far away from this one!\"\r\n",
      "\"__label__book The Worst! A complete waste of time. Typographical errors, poor grammar, and a totally pathetic plot add up to absolutely nothing. I'm embarrassed for this author and very disappointed I actually paid for this book.\"\r\n",
      "\"__label__book Oh please I guess you have to be a romance novel lover for this one, and not a very discerning one. All others beware! It is absolute drivel. I figured I was in trouble when a typo is prominently featured on the back cover, but the first page of the book removed all doubt. Wait - maybe I'm missing the point. A quick re-read of the beginning now makes it clear. This has to be an intentional churning of over-heated prose for satiric purposes. Phew, so glad I didn't waste $10.95 after all.\"\r\n"
     ]
    }
   ],
   "source": [
    "!head reviews.train.csv -n 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"__label__movie Not worth seeing My wife and I tried, really tried to watch this movie, and it went nowhere. It was probably the biggest disappointment we've seen in years. Boring, and so s-l-o-w you feel like cutting off your own limbs for entertainment. We give this an all toes down.\"\r\n",
      "\"__label__book Black comedy at its worst Jon Keyes directed this black-humored and odd black comedy that is not scary or funny but dumb and dull with unlikable characters,a badly written plot,and a very low budget. The movie is very reminiscent to Paul Bartel's Eating Raoul and Bob Balaban's Parents but the movie is a couple that argue about everything and then the wife gets mad and harasses a teenage girl in the basement and then tries to kill her husband as well as the husband trying to kill her. For a low-budget black comedy this is a unwatchable mess with no redeeming values. Despite the title the movie takes place in the couple's dining room,kitchen,and the living room and the front cover has nothing to do with this movie because it is not a slasher flick but a bizarre soap opera-ish movie that looks like a Lifetime movie.\"\r\n",
      "\"__label__movie What the...????? Probably the lamest waste of my money. Wait... the lamest waste of time making, production money, etc... Except for a couple of cute guys in the film, this movie was an absolute just waste. The production value is worse than those movies I made in Junior High. Is there a way to do NO STARS? Like maybe negative stars?\"\r\n"
     ]
    }
   ],
   "source": [
    "!head reviews.test.csv  -n 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the nltk tokenizer and other libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from random import shuffle\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool\n",
    "import csv\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_instance(row):\n",
    "    cur_row = []\n",
    "    cur_row.extend(nltk.word_tokenize(row[0].lower()))\n",
    "    return cur_row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `transform_instance` will be applied to each data instance in parallel using python's multiprocessing module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(input_file, output_file, keep=1):\n",
    "    all_rows = []\n",
    "    with open(input_file, 'r') as csvinfile:\n",
    "        csv_reader = csv.reader(csvinfile, delimiter=',')\n",
    "        for row in csv_reader:\n",
    "            all_rows.append(row)\n",
    "    shuffle(all_rows)\n",
    "    all_rows = all_rows[:int(keep*len(all_rows))]\n",
    "    pool = Pool(processes=multiprocessing.cpu_count())\n",
    "    transformed_rows = pool.map(transform_instance, all_rows)\n",
    "    pool.close() \n",
    "    pool.join()\n",
    "    \n",
    "    with open(output_file, 'w') as csvoutfile:\n",
    "        csv_writer = csv.writer(csvoutfile, delimiter=' ', lineterminator='\\n')\n",
    "        csv_writer.writerows(transformed_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22.3 s, sys: 3.04 s, total: 25.4 s\n",
      "Wall time: 3min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Preparing the training dataset\n",
    "preprocess('reviews.train.csv', 'reviews.train', keep=1)\n",
    "        \n",
    "# Preparing the validation dataset        \n",
    "preprocess('reviews.test.csv', 'reviews.validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__label__movie a horrible movie of titanic proportions this animated adaption of this so-called `` movie '' should not be seen by the human eye . it 's because of the infamous traumatizing , horrible `` rapping dog '' scene . it rots both your brain and your very innocent soul . this movie should sink into the depths of the ocean like the titanic .\r\n",
      "__label__music soundgarden fan hes an awesome singser but he needs the guys from audioslave or soundgarden to back him up . not that he needs it but when they are together they make the best music known ! ! !\r\n",
      "__label__review prejudice on parade this book is not a work of history . it is the painful autobiography of an excommnicated ex-priest on why he hates the catholic church.as the end of the book makes perfectly clear , the author hates the church 's teaching on revelation , morality , authority , sexuality -- -just about everything . it is in light of this ardent hate and resentment that the author risibly distorts the record on the catholic church on judaism . evidence is exaggerated and suppressed to suit the author 's bitter pejudices . evidence is even concocted , such as the absurd story of john xxiii criticizing pius xii for his alleged `` silence . '' in fact , john xxiii had nothing but praise for pius xii , who personally approved the future john xxiii 's rescue work for jews in turkey during world war ii.a scholarly embarrassment motivated by raw anti-catholicism .\r\n"
     ]
    }
   ],
   "source": [
    "!head reviews.train  -n 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"__label__movie Not worth seeing My wife and I tried, really tried to watch this movie, and it went nowhere. It was probably the biggest disappointment we've seen in years. Boring, and so s-l-o-w you feel like cutting off your own limbs for entertainment. We give this an all toes down.\"\r\n",
      "\"__label__book Black comedy at its worst Jon Keyes directed this black-humored and odd black comedy that is not scary or funny but dumb and dull with unlikable characters,a badly written plot,and a very low budget. The movie is very reminiscent to Paul Bartel's Eating Raoul and Bob Balaban's Parents but the movie is a couple that argue about everything and then the wife gets mad and harasses a teenage girl in the basement and then tries to kill her husband as well as the husband trying to kill her. For a low-budget black comedy this is a unwatchable mess with no redeeming values. Despite the title the movie takes place in the couple's dining room,kitchen,and the living room and the front cover has nothing to do with this movie because it is not a slasher flick but a bizarre soap opera-ish movie that looks like a Lifetime movie.\"\r\n",
      "\"__label__movie What the...????? Probably the lamest waste of my money. Wait... the lamest waste of time making, production money, etc... Except for a couple of cute guys in the film, this movie was an absolute just waste. The production value is worse than those movies I made in Junior High. Is there a way to do NO STARS? Like maybe negative stars?\"\r\n"
     ]
    }
   ],
   "source": [
    "!head reviews.test  -n 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data preprocessing cell might take a minute to run. After the data preprocessing is complete, we need to upload it to S3 so that it can be consumed by SageMaker to execute training jobs. We'll use Python SDK to upload these two files to the bucket and prefix location that we have set above.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.55 s, sys: 1.77 s, total: 4.32 s\n",
      "Wall time: 4.51 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_channel = prefix + '/train'\n",
    "validation_channel = prefix + '/validation'\n",
    "\n",
    "sess.upload_data(path='reviews.train', bucket=bucket, key_prefix=train_channel)\n",
    "sess.upload_data(path='reviews.validation', bucket=bucket, key_prefix=validation_channel)\n",
    "\n",
    "s3_train_data = 's3://{}/{}'.format(bucket, train_channel)\n",
    "s3_validation_data = 's3://{}/{}'.format(bucket, validation_channel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to setup an output location at S3, where the model artifact will be dumped. These artifacts are also the output of the algorithm's traning job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_output_location = 's3://{}/{}/output'.format(bucket, prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "Now that we are done with all the setup that is needed, we are ready to train our reviews muliti classifier. To begin, let us create a ``sageMaker.estimator.Estimator`` object. This estimator will launch the training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_name = boto3.Session().region_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using SageMaker BlazingText container: 811284229777.dkr.ecr.us-east-1.amazonaws.com/blazingtext:latest (us-east-1)\n"
     ]
    }
   ],
   "source": [
    "container = sagemaker.amazon.amazon_estimator.get_image_uri(region_name, \"blazingtext\", \"latest\")\n",
    "print('Using SageMaker BlazingText container: {} ({})'.format(container, region_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the BlazingText model for supervised text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "bt_model = sagemaker.estimator.Estimator(container,\n",
    "                                         role, \n",
    "                                         base_job_name='phdataBlazingText',\n",
    "                                         train_instance_count=1, \n",
    "                                         train_instance_type='ml.c4.4xlarge',\n",
    "                                         train_volume_size = 30,\n",
    "                                         train_max_run = 360000,\n",
    "                                         input_mode= 'File',\n",
    "                                         output_path=s3_output_location,\n",
    "                                         sagemaker_session=sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For complete list of hyperparameters, refer to [algorithm documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/blazingtext_hyperparameters.html) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "bt_model.set_hyperparameters(mode=\"supervised\",\n",
    "                            epochs=10,\n",
    "                            min_count=2,\n",
    "                            learning_rate=0.05,\n",
    "                            vector_dim=10,\n",
    "                            early_stopping=True,\n",
    "                            patience=4,\n",
    "                            min_epochs=5,\n",
    "                            word_ngrams=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the hyper-parameters are setup, let us prepare the handshake between our data channels and the algorithm. To do this, we need to create the `sagemaker.session.s3_input` objects from our data channels. These objects are then put in a simple dictionary, which the algorithm consumes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = sagemaker.session.s3_input(s3_train_data, distribution='FullyReplicated', \n",
    "                        content_type='text/plain', s3_data_type='S3Prefix')\n",
    "validation_data = sagemaker.session.s3_input(s3_validation_data, distribution='FullyReplicated', \n",
    "                             content_type='text/plain', s3_data_type='S3Prefix')\n",
    "data_channels = {'train': train_data, 'validation': validation_data}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have our `Estimator` object, we have set the hyper-parameters for this object and we have our data channels linked with the algorithm. The only  remaining thing to do is to train the algorithm. The following command will train the algorithm. Training the algorithm involves a few steps. Firstly, the instance that we requested while creating the `Estimator` classes is provisioned and is setup with the appropriate libraries. Then, the data from our channels are downloaded into the instance. Once this is done, the training job begins. The provisioning and data downloading will take some time, depending on the size of the data. Therefore it might be a few minutes before we start getting training logs for our training jobs. The data logs will also print out Accuracy on the validation data for every epoch after training job has executed `min_epochs`. This metric is a proxy for the quality of the algorithm. \n",
    "\n",
    "Once the job has finished a \"Job complete\" message will be printed. The trained model can be found in the S3 bucket that was setup as `output_path` in the estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-29 23:53:47 Starting - Starting the training job...\n",
      "2020-01-29 23:53:48 Starting - Launching requested ML instances......\n",
      "2020-01-29 23:54:49 Starting - Preparing the instances for training...\n",
      "2020-01-29 23:55:43 Downloading - Downloading input data...\n",
      "2020-01-29 23:56:10 Training - Training image download completed. Training in progress..\u001b[34mArguments: train\u001b[0m\n",
      "\u001b[34m[01/29/2020 23:56:11 WARNING 139977443522368] Loggers have already been setup.\u001b[0m\n",
      "\u001b[34m[01/29/2020 23:56:11 WARNING 139977443522368] Loggers have already been setup.\u001b[0m\n",
      "\u001b[34m[01/29/2020 23:56:11 INFO 139977443522368] nvidia-smi took: 0.0251870155334 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[34m[01/29/2020 23:56:11 INFO 139977443522368] Running single machine CPU BlazingText training using supervised mode.\u001b[0m\n",
      "\u001b[34m[01/29/2020 23:56:11 INFO 139977443522368] Processing /opt/ml/input/data/train/reviews.train . File size: 186 MB\u001b[0m\n",
      "\u001b[34m[01/29/2020 23:56:11 INFO 139977443522368] Processing /opt/ml/input/data/validation/reviews.validation . File size: 79 MB\u001b[0m\n",
      "\u001b[34mRead 10M words\u001b[0m\n",
      "\u001b[34mRead 20M words\u001b[0m\n",
      "\u001b[34mRead 30M words\u001b[0m\n",
      "\u001b[34mRead 39M words\u001b[0m\n",
      "\u001b[34mNumber of words:  151682\u001b[0m\n",
      "\u001b[34mLoading validation data from /opt/ml/input/data/validation/reviews.validation\u001b[0m\n",
      "\u001b[34mLoaded validation data.\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 4\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0291  Progress: 41.78%  Million Words/sec: 37.77 #####\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0266  Progress: 46.87%  Million Words/sec: 37.99 #####\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 5\u001b[0m\n",
      "\u001b[34mUsing 16 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.693548\u001b[0m\n",
      "\u001b[34mValidation accuracy improved! Storing best weights...\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0238  Progress: 52.40%  Million Words/sec: 36.48 #####\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0213  Progress: 57.50%  Million Words/sec: 36.77 #####\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 6\u001b[0m\n",
      "\u001b[34mUsing 16 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.693182\u001b[0m\n",
      "\u001b[34mValidation accuracy has not improved for last 1 epochs.\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0183  Progress: 63.40%  Million Words/sec: 35.69 #####\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0158  Progress: 68.47%  Million Words/sec: 35.97 #####\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 7\u001b[0m\n",
      "\u001b[34mUsing 16 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.695826\u001b[0m\n",
      "\u001b[34mValidation accuracy improved! Storing best weights...\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0130  Progress: 73.96%  Million Words/sec: 35.14 #####\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0105  Progress: 79.04%  Million Words/sec: 35.42 #####\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 8\u001b[0m\n",
      "\u001b[34mUsing 16 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.696559\u001b[0m\n",
      "\u001b[34mValidation accuracy improved! Storing best weights...\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0077  Progress: 84.51%  Million Words/sec: 34.74 #####\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0052  Progress: 89.59%  Million Words/sec: 34.99 #####\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 9\u001b[0m\n",
      "\u001b[34mUsing 16 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.696037\u001b[0m\n",
      "\u001b[34mValidation accuracy has not improved for last 1 epochs.\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0027  Progress: 94.60%  Million Words/sec: 34.34 #####\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0002  Progress: 99.69%  Million Words/sec: 34.58 #####\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 10\u001b[0m\n",
      "\u001b[34mUsing 16 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.697843\u001b[0m\n",
      "\u001b[34mValidation accuracy improved! Storing best weights...\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0000  Progress: 100.00%  Million Words/sec: 33.83 #####\u001b[0m\n",
      "\n",
      "2020-01-29 23:56:40 Uploading - Uploading generated training model\u001b[34mTraining finished.\u001b[0m\n",
      "\u001b[34mAverage throughput in Million words/sec: 33.83\u001b[0m\n",
      "\u001b[34mTotal training time in seconds: 11.62\n",
      "\u001b[0m\n",
      "\u001b[34m#train_accuracy: 0.8082\u001b[0m\n",
      "\u001b[34mNumber of train examples: 420001\n",
      "\u001b[0m\n",
      "\u001b[34m#validation_accuracy: 0.6978\u001b[0m\n",
      "\u001b[34mNumber of validation examples: 179999\u001b[0m\n",
      "\n",
      "2020-01-29 23:56:57 Completed - Training job completed\n",
      "Training seconds: 74\n",
      "Billable seconds: 74\n"
     ]
    }
   ],
   "source": [
    "bt_model.fit(inputs=data_channels, logs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hosting / Inference\n",
    "Once the training is done, we can deploy the trained model as an Amazon SageMaker real-time hosted endpoint. This will allow us to make predictions (or inference) from the model. Note that we don't have to host on the same type of instance that we used to train. Because instance endpoints will be up and running for long, it's advisable to choose a cheaper instance for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------!"
     ]
    }
   ],
   "source": [
    "text_classifier = bt_model.deploy(initial_instance_count = 1,instance_type = 'ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_classifier = sagemaker.predictor.RealTimePredictor(endpoint='phdataBlazingText-2020-01-29-23-53-47-082', sagemaker_session=sess)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use JSON format for inference\n",
    "BlazingText supports `application/json` as the content-type for inference. The payload should contain a list of sentences with the key as \"**instances**\" while being passed to the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"prob\": [\n",
      "      0.8382528424263\n",
      "    ],\n",
      "    \"label\": [\n",
      "      \"__label__movie\"\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"prob\": [\n",
      "      0.96994948387146\n",
      "    ],\n",
      "    \"label\": [\n",
      "      \"__label__movie\"\n",
      "    ]\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "sentences = [\"Not worth seeing My wife and I tried, really tried to watch this movie, and it went nowhere. It was probably the biggest disappointment we've seen in years. Boring, and so s-l-o-w you feel like cutting off your own limbs for entertainment. We give this an all toes down\", \"comedy at its worst Jon Keyes directed this black-humored and odd black comedy that is not scary or funny but dumb and dull with unlikable characters,a badly written plot,and a very low budget. The movie is very reminiscent to Paul Bartel's Eating Raoul and Bob Balaban's Parents but the movie is a couple that argue about everything and then the wife gets mad and harasses a teenage girl in the basement and then tries to kill her husband as well as the husband trying to kill her. For a low-budget black comedy this is a unwatchable mess with no redeeming values. Despite the title the movie takes place in the couple's dining room,kitchen,and the living room and the front cover has nothing to do with this movie because it is not a slasher flick but a bizarre soap opera-ish movie that looks like a Lifetime movie.\"] \n",
    "             \n",
    "             \n",
    "\n",
    "             \n",
    "             \n",
    "# using the same nltk tokenizer that we used during data preparation for training\n",
    "tokenized_sentences = [' '.join(nltk.word_tokenize(sent)) for sent in sentences]\n",
    "\n",
    "payload = {\"instances\" : tokenized_sentences}\n",
    "\n",
    "response = text_classifier.predict(json.dumps(payload))\n",
    "\n",
    "predictions = json.loads(response)\n",
    "print(json.dumps(predictions, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the model will return only one prediction, the one with the highest probability. For retrieving the top k predictions, we can set `k` in the configuration as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"prob\": [\n",
      "      0.8382528424263,\n",
      "      0.07862398773431778\n",
      "    ],\n",
      "    \"label\": [\n",
      "      \"__label__movie\",\n",
      "      \"__label__quality\"\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"prob\": [\n",
      "      0.96994948387146,\n",
      "      0.015944872051477432\n",
      "    ],\n",
      "    \"label\": [\n",
      "      \"__label__movie\",\n",
      "      \"__label__review\"\n",
      "    ]\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "payload = {\"instances\" : tokenized_sentences,\n",
    "          \"configuration\": {\"k\": 2}}\n",
    "\n",
    "response = text_classifier.predict(json.dumps(payload))\n",
    "\n",
    "predictions = json.loads(response)\n",
    "print(json.dumps(predictions, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop / Close the Endpoint \n",
    "Finally, we should delete the endpoint before we close the notebook if we don't need to keep the endpoint running for serving realtime predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sess.delete_endpoint(text_classifier.endpoint)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "notice": "Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
